{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Wrap each document in the Document class\n",
    "documents = [\n",
    "    Document(page_content=\"Climate change affects polar bear habitats by reducing ice cover, making it harder for them to hunt seals.\"),\n",
    "    Document(page_content=\"Rising global temperatures have led to the melting of polar ice caps, directly impacting wildlife in these regions.\"),\n",
    "    Document(page_content=\"Various conservation efforts aim to protect polar bear populations by establishing protected areas and reducing greenhouse gas emissions.\"),\n",
    "    Document(page_content=\"Climate change has far-reaching effects on ecosystems, impacting food chains and biodiversity on a global scale.\"),\n",
    "    Document(page_content=\"Polar bears rely on sea ice for hunting, resting, and breeding. Loss of sea ice due to global warming forces them to travel farther for food.\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "def standard_rag(documents, query, embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\", llm_model_name=\"EleutherAI/gpt-neo-125M\", max_new_tokens=100, chain_type=\"stuff\"):\n",
    "    \"\"\"\n",
    "    Implements a standard RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of `Document` objects containing page content.\n",
    "        query (str): The query to retrieve information for.\n",
    "        embedding_model_name (str): The name of the embedding model for vector storage. Default is \"sentence-transformers/all-MiniLM-L6-v2\".\n",
    "        llm_model_name (str): The name of the language model for text generation. Default is \"EleutherAI/gpt-neo-125M\".\n",
    "        max_new_tokens (int): The maximum number of tokens for generation. Default is 100.\n",
    "        chain_type (str): The chain type to use for the RetrievalQA. Default is \"stuff\".\n",
    "        \n",
    "    Returns:\n",
    "        str: The response generated for the query.\n",
    "    \"\"\"\n",
    "    # Set up HuggingFaceEmbeddings with the specified model\n",
    "    hf_embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "    # Create the FAISS vector store using Document-wrapped documents\n",
    "    vector_store = FAISS.from_documents(documents, hf_embeddings)\n",
    "\n",
    "    # Define the retriever from the vector store\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    # Set up the generation pipeline\n",
    "    generation_pipeline = pipeline(\"text-generation\", model=llm_model_name, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Wrap the pipeline in LangChain as an LLM\n",
    "    llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
    "\n",
    "    # Create the RetrievalQA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=chain_type\n",
    "    )\n",
    "\n",
    "    # Run the query through the chain and return the response\n",
    "    response = qa_chain.run(query)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Spandana\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spandana\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Spandana\\miniconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Polar bears are the most vulnerable species in the world.\n",
      "\n",
      "Climate change affects polar bear habitats by reducing ice cover, making it harder for them to hunt seals.\n",
      "\n",
      "Polar bears rely on sea ice for hunting, resting, and breeding. Loss of sea ice due to global warming forces them to travel farther for food.\n",
      "\n",
      "Various conservation efforts aim to protect polar bear populations by establishing protected areas and reducing greenhouse gas emissions.\n",
      "\n",
      "Rising global temperatures have led to the melting of polar\n"
     ]
    }
   ],
   "source": [
    "# Define the query\n",
    "query = \"What is the impact of climate change on polar bears?\"\n",
    "\n",
    "# Call the function\n",
    "response = standard_rag(\n",
    "    documents=documents,\n",
    "    query=query\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrective_rag(documents: List[Document], query: str, embedding_model_name: str, llm_model_name: str):\n",
    "    \"\"\"\n",
    "    Implements a Corrective Retrieval-Augmented Generation pipeline.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize embeddings and vector store\n",
    "    hf_embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    vector_store = FAISS.from_documents(documents, hf_embeddings)\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    # Step 2: Initialize the LLM for generation and semantic similarity\n",
    "    generation_pipeline = pipeline(\"text-generation\", model=llm_model_name, max_new_tokens=100)\n",
    "    llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
    "\n",
    "    def grade_relevance(query: str, retrieved_docs: List[Document]) -> bool:\n",
    "        \"\"\"\n",
    "        Uses semantic similarity to grade the relevance of retrieved documents.\n",
    "        \"\"\"\n",
    "        # Concatenate retrieved document content\n",
    "        context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "        grading_prompt = (\n",
    "            f\"Determine if the following context is relevant to the query:\\n\\n\"\n",
    "            f\"Query: {query}\\nContext: {context}\\n\\n\"\n",
    "            f\"Respond with 'yes' or 'no'.\"\n",
    "        )\n",
    "        # Generate relevance grading using the LLM\n",
    "        relevance_result = llm.generate([grading_prompt])\n",
    "        relevance_response = relevance_result.generations[0][0].text  # Extract the first generation's text\n",
    "        return \"yes\" in relevance_response.lower()\n",
    "\n",
    "    def reflect_and_improve(response: str, query: str, retrieved_docs: List[Document]) -> str:\n",
    "        \"\"\"\n",
    "        Implements a reflection mechanism to refine the output.\n",
    "        \"\"\"\n",
    "        reflection_prompt = (\n",
    "            f\"Given the user's query: {query}\\n\\n\"\n",
    "            f\"The initial response: {response}\\n\\n\"\n",
    "            f\"Using the context: {' '.join([doc.page_content for doc in retrieved_docs])}, \"\n",
    "            f\"improve the response to ensure it fully addresses the query.\"\n",
    "        )\n",
    "        # Generate refined response using the LLM\n",
    "        reflection_result = llm.generate([reflection_prompt])\n",
    "        refined_response = reflection_result.generations[0][0].text  # Extract the first generation's text\n",
    "        return refined_response\n",
    "\n",
    "    # Step 3: Retrieve initial documents\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Step 4: Check document relevance\n",
    "    if not grade_relevance(query, retrieved_docs):\n",
    "        # Rewrite the query for improved relevance\n",
    "        refined_query = f\"Provide more detailed context about: {query}\"\n",
    "        retrieved_docs = retriever.get_relevant_documents(refined_query)\n",
    "\n",
    "    # Step 5: Generate initial response\n",
    "    retrieval_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type=\"stuff\")\n",
    "    initial_response = retrieval_chain.run(query)\n",
    "\n",
    "    # Step 6: Reflect and improve the response\n",
    "    final_response = reflect_and_improve(initial_response, query, retrieved_docs)\n",
    "\n",
    "    return final_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Spandana\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Polar bears rely on sea ice for hunting, resting, and breeding. Loss of sea ice due to global warming forces them to travel farther for food.\n",
      "\n",
      "Polar bears rely on sea ice for hunting, resting, and breeding. Loss of sea ice due to global warming forces them to travel farther for food.\n",
      "\n",
      "Polar bears rely on sea ice for hunting, resting, and breeding. Loss of sea ice due to global warming forces them to travel farther for food.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the impact of climate change on polar bears?\"\n",
    "\n",
    "response = corrective_rag(\n",
    "    documents=documents,\n",
    "    query=query,\n",
    "    embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    llm_model_name=\"EleutherAI/gpt-neo-125M\"\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "Vector Store: FAISS\n",
    "\n",
    "retriever: FAISS\n",
    "\n",
    "LLM: gpt-neo-125M\n",
    "\n",
    "framework: Langchain\n",
    "\n",
    "\n",
    "types of RAG:  Standard RAG, Corrective RAG, Fusion RAG, Self RAG, Agentic RAG."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
